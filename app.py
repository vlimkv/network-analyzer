import streamlit as st
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
import plotly.graph_objects as go
import plotly.figure_factory as ff
from st_aggrid import AgGrid, GridOptionsBuilder
import os

os.environ["STREAMLIT_SERVER_MAX_UPLOAD_SIZE"] = "1024"

pd.set_option("styler.render.max_elements", 3000000)
st.set_page_config(layout="wide")

@st.cache_resource
def load_models():
    rf_model = joblib.load("src/rf_model_fixed.pkl")
    lstm_model = tf.keras.models.load_model("src/lstm_model.h5")
    scaler_mean = np.load("src/scaler_lstm.npy")
    return rf_model, lstm_model, scaler_mean

rf_model, lstm_model, scaler_mean = load_models()

st.title("üõ°Ô∏è –°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞ (–≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å RF+LSTM)")
uploaded_file = st.file_uploader("üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV, Excel –∏–ª–∏ Parquet —Ñ–∞–π–ª", type=["csv", "xlsx", "parquet"])

if uploaded_file is not None:
    if uploaded_file.name.endswith(".csv"):
        data = pd.read_csv(uploaded_file)
    elif uploaded_file.name.endswith(".xlsx"):
        data = pd.read_excel(uploaded_file)
    elif uploaded_file.name.endswith(".parquet"):
        data = pd.read_parquet(uploaded_file)
    else:
        st.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞")
        st.stop()

    st.subheader("üìã –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
    st.dataframe(data.head())

    required_columns = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment']
    if not all(col in data.columns for col in required_columns):
        st.error(f"‚ùå –í —Ñ–∞–π–ª–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {', '.join(required_columns)}")
        st.stop()

    X = data[required_columns].values
    if len(required_columns) != scaler_mean.shape[0]:
        st.error("‚õîÔ∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏")
        st.stop()

    X_norm = (X - scaler_mean) / np.std(X, axis=0)
    X_lstm = X_norm.reshape((X_norm.shape[0], 1, X_norm.shape[1]))

    rf_preds = rf_model.predict(X)
    lstm_preds = (lstm_model.predict(X_lstm) > 0.5).astype(int).flatten()
    hybrid_preds = (rf_preds + lstm_preds) // 2

    results_df = data.copy()
    results_df['RF'] = rf_preds
    results_df['LSTM'] = lstm_preds
    results_df['Hybrid'] = hybrid_preds
    results_df['Status'] = results_df['Hybrid'].map({0: '‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω–æ', 1: '‚ö†Ô∏è –ê—Ç–∞–∫–∞'})

    tab1, tab2 = st.tabs(["üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã", "üìà –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π"])

    with tab1:
        st.subheader(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ (–≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(results_df)})")
        gb = GridOptionsBuilder.from_dataframe(results_df)
        gb.configure_pagination()
        gb.configure_default_column(filter=True, resizable=True, sortable=True)
        AgGrid(results_df, gridOptions=gb.build())

        st.subheader("üìå –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        col1, col2 = st.columns(2)
        col1.metric("‚ö†Ô∏è –ê—Ç–∞–∫", int(hybrid_preds.sum()))
        col2.metric("‚úÖ –ù–æ—Ä–º", int((hybrid_preds == 0).sum()))
        st.bar_chart(results_df['Hybrid'].value_counts())

        csv = results_df.to_csv(index=False).encode('utf-8')
        st.download_button("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", csv, "prediction_result.csv", "text/csv")

    with tab2:
        st.subheader("üìà –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")

        if 'binary_label' in data.columns:
            y_true = data['binary_label'].values

            def plot_roc_interactive(fpr, tpr, auc, model_name):
                fig = go.Figure()
                fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f"AUC = {auc:.2f}"))
                fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], line=dict(dash='dash'), name='baseline'))
                fig.update_layout(title=f"ROC-–∫—Ä–∏–≤–∞—è: {model_name}", xaxis_title='FPR', yaxis_title='TPR')
                st.plotly_chart(fig)

            def plot_confusion_matrix_interactive(cm, labels, model_name):
                z = cm.tolist()
                fig = ff.create_annotated_heatmap(
                    z, x=labels, y=labels, colorscale='Blues', showscale=True,
                    annotation_text=[[str(val) for val in row] for row in z], font_colors=["black"]
                )
                fig.update_layout(
                    title=f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: {model_name}",
                    xaxis_title="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ",
                    yaxis_title="–ò—Å—Ç–∏–Ω–∞"
                )
                st.plotly_chart(fig, use_container_width=True)

            def show_metrics(y_true, y_pred, model_name):
                st.markdown(f"### üìå {model_name}")
                df_report = pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)).transpose()
                st.dataframe(df_report)

                cm = confusion_matrix(y_true, y_pred)
                plot_confusion_matrix_interactive(cm, ["–ù–æ—Ä–º–∞", "–ê—Ç–∞–∫–∞"], model_name)

                fpr, tpr, _ = roc_curve(y_true, y_pred)
                auc = roc_auc_score(y_true, y_pred)
                plot_roc_interactive(fpr, tpr, auc, model_name)

            show_metrics(y_true, rf_preds, "Random Forest")
            show_metrics(y_true, lstm_preds, "LSTM")
            show_metrics(y_true, hybrid_preds, "–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å")

            st.subheader("üèÜ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")
            models = {
                "Random Forest": rf_preds,
                "LSTM": lstm_preds,
                "–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å": hybrid_preds
            }
            comparison_data = []
            for name, y_pred in models.items():
                comparison_data.append({
                    "–ú–æ–¥–µ–ª—å": name,
                    "Accuracy": accuracy_score(y_true, y_pred),
                    "Precision": precision_score(y_true, y_pred, zero_division=0),
                    "Recall": recall_score(y_true, y_pred, zero_division=0),
                    "F1-score": f1_score(y_true, y_pred, zero_division=0),
                    "AUC": roc_auc_score(y_true, y_pred)
                })
            df_comparison = pd.DataFrame(comparison_data).set_index("–ú–æ–¥–µ–ª—å")
            st.dataframe(df_comparison.style.highlight_max(axis=0, color='lightgreen'))

        else:
            st.warning("‚ö†Ô∏è –í —Ñ–∞–π–ª–µ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ `binary_label`. –ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.")
else:
    st.info("‚¨ÜÔ∏è –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")